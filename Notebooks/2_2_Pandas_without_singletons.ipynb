{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81bb33a",
   "metadata": {},
   "source": [
    "Compiling the wordcount data into one big dataframe technically worked, but the dataframes were too big to fit onto a pandas dataframe, with each chunk of 1000 books having about 38,000 columns--and the columns didn't match up, so combining them would make the dfs even bigger. This might be worth switching from pandas to dask if all of those words had a predictive value to our model, but it seems clear that a word that appears in a book only 1 time, or that appears in only 1 book, is unlikely to help our model. \n",
    "\n",
    "In this notebook, I've dropped from each given row every word that appears in a book only once, and then experimented with dropping words that appeared in less than 0.5% of each chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba6bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799d4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../CSVs/8_filtered_genres.csv',index_col=0).drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220c68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>total words</th>\n",
       "      <th>vividness</th>\n",
       "      <th>passive voice</th>\n",
       "      <th>all adverbs</th>\n",
       "      <th>ly-adverbs</th>\n",
       "      <th>non-ly-adverbs</th>\n",
       "      <th>year</th>\n",
       "      <th>genres</th>\n",
       "      <th>num final genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Vanished Birds</td>\n",
       "      <td>Simon Jimenez</td>\n",
       "      <td>124205.0</td>\n",
       "      <td>55.18</td>\n",
       "      <td>6.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>['Science Fiction', 'Fantasy', 'Adult']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Price of Honor</td>\n",
       "      <td>Jonathan P. Brazee</td>\n",
       "      <td>77253.0</td>\n",
       "      <td>35.35</td>\n",
       "      <td>8.71</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>['Science Fiction']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Case of the Baker Street Irregulars</td>\n",
       "      <td>Anthony Boucher</td>\n",
       "      <td>80557.0</td>\n",
       "      <td>32.33</td>\n",
       "      <td>8.41</td>\n",
       "      <td>3.72</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>['Mystery', 'Crime', 'Classics']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wildoak</td>\n",
       "      <td>C. C. Harrington</td>\n",
       "      <td>55602.0</td>\n",
       "      <td>74.34</td>\n",
       "      <td>6.92</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>['Historical Fiction', 'Young Adult']</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Holiday</td>\n",
       "      <td>T. M. Logan</td>\n",
       "      <td>101767.0</td>\n",
       "      <td>50.30</td>\n",
       "      <td>8.02</td>\n",
       "      <td>3.06</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>['Thriller', 'Mystery', 'Crime', 'Suspense']</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title              author  total words  \\\n",
       "0                       The Vanished Birds       Simon Jimenez     124205.0   \n",
       "1                       The Price of Honor  Jonathan P. Brazee      77253.0   \n",
       "2  The Case of the Baker Street Irregulars     Anthony Boucher      80557.0   \n",
       "3                                  Wildoak    C. C. Harrington      55602.0   \n",
       "4                              The Holiday         T. M. Logan     101767.0   \n",
       "\n",
       "   vividness  passive voice  all adverbs  ly-adverbs  non-ly-adverbs    year  \\\n",
       "0      55.18           6.37         1.95        0.36            1.58  2020.0   \n",
       "1      35.35           8.71         2.63        0.71            1.92  2017.0   \n",
       "2      32.33           8.41         3.72        1.64            2.08  1940.0   \n",
       "3      74.34           6.92         3.04        1.16            1.87  2022.0   \n",
       "4      50.30           8.02         3.06        1.12            1.93  2019.0   \n",
       "\n",
       "                                         genres  num final genres  \n",
       "0       ['Science Fiction', 'Fantasy', 'Adult']                 3  \n",
       "1                           ['Science Fiction']                 1  \n",
       "2              ['Mystery', 'Crime', 'Classics']                 3  \n",
       "3         ['Historical Fiction', 'Young Adult']                 2  \n",
       "4  ['Thriller', 'Mystery', 'Crime', 'Suspense']                 4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "513350e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts_from_json(author, title):\n",
    "    file_path = f'../word-counts/{author}/{title}/word-counts.json'\n",
    "    word_counts = read_json(file_path)\n",
    "    # Remove words that only appear once\n",
    "    pruned_word_counts = {word: number for word, number in word_counts.items() if number > 1}\n",
    "    return pruned_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc2bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ba7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory_structure(root_dir):\n",
    "    authors = set()\n",
    "    books = defaultdict(set)\n",
    "    #df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for author in os.listdir(root_dir):\n",
    "        if os.path.isdir(f'{root_dir}/{author}'):\n",
    "            authors.add(author)\n",
    "            author_dir = os.path.join(root_dir, author)\n",
    "\n",
    "            for book in os.listdir(author_dir):\n",
    "                book_dir = os.path.join(author_dir, book)\n",
    "                books[author].add(book)\n",
    "\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fe3544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../word-counts/'\n",
    "books = process_directory_structure(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb9dafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_to_dataframe(df, author, book, word_counts):\n",
    "    word_counts['Author'] = author\n",
    "    word_counts['Book'] = book\n",
    "    df = pd.concat(word_counts, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00adede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(name):\n",
    "    to_remove = [':',',','â€™','?','/']\n",
    "    for char in to_remove:\n",
    "        name = name.replace(char, '')\n",
    "    name = name.lower().replace(' ','-').replace('&','and').replace('.-','-').replace('.','-').strip('-')\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31402a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordcounts(orig_title, orig_author, correct_books, threshold=80):\n",
    "    # Reformat title and author's name to match folder names\n",
    "    title = clean(orig_title)\n",
    "    author = clean(orig_author)\n",
    "    correct_books_copy = correct_books.copy()\n",
    "    \n",
    "    \n",
    "    # Check if the author's name is in the list of folders at the author level\n",
    "    if author in correct_books_copy.keys():\n",
    "        # If found, check the author's folder for the title folder. \n",
    "        if title in correct_books[author]:\n",
    "            return get_word_counts_from_json(author,title)\n",
    "        else: \n",
    "            # Check for title names that are close to the cleaned title name\n",
    "            for correct_title in correct_books[author]:\n",
    "                if fuzz.ratio(title, correct_title) > threshold:\n",
    "                    return get_wordcounts(correct_title, author, correct_books, threshold)\n",
    "        return -1\n",
    "    else: \n",
    "        # If the exact author's name wasn't found, check for extremely similar authors' names. \n",
    "        for correct_author in correct_books_copy.keys():\n",
    "            if fuzz.ratio(author, correct_author) > 95:\n",
    "                return get_wordcounts(title, correct_author, correct_books, threshold)\n",
    "        # If that doesn't work, check for moderately similar authors' names. \n",
    "        for correct_author in correct_books_copy.keys():\n",
    "            if fuzz.ratio(author, correct_author) > threshold:\n",
    "                return get_wordcounts(title, correct_author, correct_books, threshold)\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9fef995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the columns have names that could be words in books. Better make each column name two words.\n",
    "df.columns = ['book title', 'book author', 'total words', 'vividness score', 'passive voice',\n",
    "       'all adverbs', 'ly-adverbs', 'non-ly-adverbs', 'publication year', 'book genres',\n",
    "       'num genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed7010af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6db6055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d502ccb07949c4a1e59c4d844c39dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = df_chunk.join(df_chunk.progress_apply\\\n",
    "                        (lambda x: pd.Series(get_wordcounts(x['book title'], x['book author'], books)), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1e7c5488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 162978)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae05d520",
   "metadata": {},
   "source": [
    "It appears that dropping words that appear only once in each book has taken the number of features for the first 1000 books from 394352 to only 162978. This means that nearly 60% of the words in each book, on average, appear only once. \n",
    "\n",
    "Next, let's try removing words that appear in less than 0.1%, 0.2% ... 0.5% of books in the chunk of 1000 books.\n",
    "\n",
    "We'll take a look at the kind of words we're excluding, and the number of words that are left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "21b731c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_dfs = [test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ccf37cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,6):\n",
    "    smaller_dfs.append(test_df.dropna(axis=1,thresh=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "62b80f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A list of words that appear at least 1 times includes 162968 words.\n",
      "The first 10 words are ['0', '0-10', '0-2-1-7', '0-3', '0-99', '00', '000', '000-Year', '000-a-year', '000-acre']\n",
      "The last 10 words are [\"Ã³'cuinn\", 'Ã³ig', 'Ã´i', 'Ã¶sana', 'Ã¹', 'Ã¼', 'Ã¼ber', 'Ã¼bersecret', 'ÅŒfuna', 'ÅŒtsuka']\n",
      "\n",
      "\n",
      "A list of words that appear at least 2 times includes 76563 words.\n",
      "The first 10 words are ['0', '00', '000', '000-acre', '000-foot', '000-mile', '000-pound', '000-ton', '002', '007']\n",
      "The last 10 words are ['Ã¨', 'Ã©', 'Ã©clair', 'Ã©cus', 'Ã©migrÃ©', 'Ã©migrÃ©s', 'Ã©tait', 'Ã©toile', 'Ãªtes', 'Ã¼ber']\n",
      "\n",
      "\n",
      "A list of words that appear at least 3 times includes 59206 words.\n",
      "The first 10 words are ['0', '00', '000', '01', '02', '03', '04', '0400', '045', '05']\n",
      "The last 10 words are ['Ã‰cole', 'Ã‰mile', 'ÃŽle', 'Ã ', 'Ã§a', 'Ã¨', 'Ã©clair', 'Ã©migrÃ©', 'Ã©migrÃ©s', 'Ãªtes']\n",
      "\n",
      "\n",
      "A list of words that appear at least 4 times includes 50531 words.\n",
      "The first 10 words are ['0', '00', '000', '01', '02', '03', '04', '05', '06', '0600']\n",
      "The last 10 words are ['zoos', 'zu', 'zucchini', 'zygote', 'Ã‰cole', 'ÃŽle', 'Ã ', 'Ã§a', 'Ã©clair', 'Ã©migrÃ©']\n",
      "\n",
      "\n",
      "A list of words that appear at least 5 times includes 45013 words.\n",
      "The first 10 words are ['0', '00', '000', '01', '02', '03', '04', '05', '06', '07']\n",
      "The last 10 words are ['zooms', 'zoos', 'zucchini', 'zygote', 'Ã‰cole', 'ÃŽle', 'Ã ', 'Ã§a', 'Ã©clair', 'Ã©migrÃ©']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"A list of words that appear at least {i+1} times includes {len(smaller_dfs[i].columns)-10} words.\")\n",
    "    print(f\"The first 10 words are {list(smaller_dfs[i].columns[11:21])}\")\n",
    "    print(f\"The last 10 words are {list(smaller_dfs[i].columns[-10:])}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7c7cd",
   "metadata": {},
   "source": [
    "It appears that nothing of likely predictive value is being lost, and we've reduced the number of features by about a factor of 10. Time to create the complete dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "74cb2efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = []\n",
    "for i in range(20):\n",
    "    df_chunks.append(df.iloc[1000*i:1000*(i+1)])\n",
    "df_chunks.append(df.iloc[20000:len(df)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c1049830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book title</th>\n",
       "      <th>book author</th>\n",
       "      <th>total words</th>\n",
       "      <th>vividness score</th>\n",
       "      <th>passive voice</th>\n",
       "      <th>all adverbs</th>\n",
       "      <th>ly-adverbs</th>\n",
       "      <th>non-ly-adverbs</th>\n",
       "      <th>publication year</th>\n",
       "      <th>book genres</th>\n",
       "      <th>num genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>Saint Death's Daughter</td>\n",
       "      <td>C. S. E. Cooney</td>\n",
       "      <td>193320.0</td>\n",
       "      <td>64.23</td>\n",
       "      <td>6.36</td>\n",
       "      <td>3.28</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>['Fantasy', 'Young Adult', 'Adult']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20001</th>\n",
       "      <td>Resonant Abyss</td>\n",
       "      <td>Chaney Hopper</td>\n",
       "      <td>90352.0</td>\n",
       "      <td>43.71</td>\n",
       "      <td>7.04</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>['Science Fiction']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20002</th>\n",
       "      <td>Creation</td>\n",
       "      <td>Gore Vidal</td>\n",
       "      <td>234303.0</td>\n",
       "      <td>35.99</td>\n",
       "      <td>8.73</td>\n",
       "      <td>3.68</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.32</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>['Historical Fiction', 'Historical', 'Classics']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20003</th>\n",
       "      <td>Dark Island</td>\n",
       "      <td>Matt James</td>\n",
       "      <td>77153.0</td>\n",
       "      <td>50.86</td>\n",
       "      <td>7.38</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>['Thriller', 'Adventure', 'Horror', 'Science F...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20004</th>\n",
       "      <td>Sword of Shadows</td>\n",
       "      <td>Jeri Westerson</td>\n",
       "      <td>83121.0</td>\n",
       "      <td>48.51</td>\n",
       "      <td>7.96</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>['Mystery', 'Historical Fiction', 'Historical']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20485</th>\n",
       "      <td>Blessing in Disguise</td>\n",
       "      <td>Danielle Steel</td>\n",
       "      <td>82682.0</td>\n",
       "      <td>25.47</td>\n",
       "      <td>10.01</td>\n",
       "      <td>3.24</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>['Romance']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20486</th>\n",
       "      <td>Moth</td>\n",
       "      <td>James Sallis</td>\n",
       "      <td>56267.0</td>\n",
       "      <td>52.27</td>\n",
       "      <td>7.19</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>['Mystery', 'Crime', 'Thriller']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20487</th>\n",
       "      <td>True Blue</td>\n",
       "      <td>Jane Smiley</td>\n",
       "      <td>73746.0</td>\n",
       "      <td>50.61</td>\n",
       "      <td>8.61</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>['Young Adult', 'Historical Fiction']</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20488</th>\n",
       "      <td>Cage of Glass</td>\n",
       "      <td>Genevieve Crownson</td>\n",
       "      <td>70143.0</td>\n",
       "      <td>49.93</td>\n",
       "      <td>7.62</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Young Adult', 'Science Fiction']</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20489</th>\n",
       "      <td>The Singing Sands</td>\n",
       "      <td>Josephine Tey</td>\n",
       "      <td>68986.0</td>\n",
       "      <td>40.43</td>\n",
       "      <td>9.29</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>['Mystery', 'Crime', 'Thriller']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   book title         book author  total words  \\\n",
       "20000  Saint Death's Daughter     C. S. E. Cooney     193320.0   \n",
       "20001          Resonant Abyss       Chaney Hopper      90352.0   \n",
       "20002                Creation          Gore Vidal     234303.0   \n",
       "20003             Dark Island          Matt James      77153.0   \n",
       "20004        Sword of Shadows      Jeri Westerson      83121.0   \n",
       "...                       ...                 ...          ...   \n",
       "20485    Blessing in Disguise     Danielle Steel       82682.0   \n",
       "20486                    Moth        James Sallis      56267.0   \n",
       "20487               True Blue         Jane Smiley      73746.0   \n",
       "20488           Cage of Glass  Genevieve Crownson      70143.0   \n",
       "20489       The Singing Sands       Josephine Tey      68986.0   \n",
       "\n",
       "       vividness score  passive voice  all adverbs  ly-adverbs  \\\n",
       "20000            64.23           6.36         3.28        1.24   \n",
       "20001            43.71           7.04         3.04        0.89   \n",
       "20002            35.99           8.73         3.68        1.35   \n",
       "20003            50.86           7.38         3.40        1.51   \n",
       "20004            48.51           7.96         2.89        0.96   \n",
       "...                ...            ...          ...         ...   \n",
       "20485            25.47          10.01         3.24        1.03   \n",
       "20486            52.27           7.19         3.37        1.10   \n",
       "20487            50.61           8.61         3.21        0.56   \n",
       "20488            49.93           7.62         3.11        1.23   \n",
       "20489            40.43           9.29         2.78        0.78   \n",
       "\n",
       "       non-ly-adverbs  publication year  \\\n",
       "20000            2.04            2022.0   \n",
       "20001            2.14            2019.0   \n",
       "20002            2.32            1981.0   \n",
       "20003            1.88            2018.0   \n",
       "20004            1.93            2020.0   \n",
       "...               ...               ...   \n",
       "20485            2.22            2019.0   \n",
       "20486            2.27            1993.0   \n",
       "20487            2.65            2011.0   \n",
       "20488            1.88               NaN   \n",
       "20489            2.00            1952.0   \n",
       "\n",
       "                                             book genres  num genres  \n",
       "20000                ['Fantasy', 'Young Adult', 'Adult']           3  \n",
       "20001                                ['Science Fiction']           1  \n",
       "20002   ['Historical Fiction', 'Historical', 'Classics']           3  \n",
       "20003  ['Thriller', 'Adventure', 'Horror', 'Science F...           4  \n",
       "20004    ['Mystery', 'Historical Fiction', 'Historical']           3  \n",
       "...                                                  ...         ...  \n",
       "20485                                        ['Romance']           1  \n",
       "20486                   ['Mystery', 'Crime', 'Thriller']           3  \n",
       "20487              ['Young Adult', 'Historical Fiction']           2  \n",
       "20488                 ['Young Adult', 'Science Fiction']           2  \n",
       "20489                   ['Mystery', 'Crime', 'Thriller']           3  \n",
       "\n",
       "[490 rows x 11 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c13bad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5ecb034a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c14c073c30c4c45898cf15efb3b7042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a3fae7049c40fd92fab7df48d3b4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2442199c65674349b99de228707df1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cc598b83de40c1acf00f0985946cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c865a1555645b5a4940875821cddc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484dbc4f5eb1427daea4c4021b960dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11340cb5ff484ffba41ac3fb8f535975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffecc5fc7da4880887a0bd320105140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c455f618994710b600b08b521bb480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bc009091414431895e6e061094351a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f98003870a44cdb13dc22d4402832d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14b55ca8dec4896a11746aba43aa819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25f114dc1d34e5f9311223eeddae2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099e888e515a42ffbaf29810593449d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2e03a6fca84ed2a2d213e2957c4d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c835e854973348b09291f8820e3729e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a2ee854f144b05af5c9c0061fe261c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee57f5d96f94496b01b5f420a5b32eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc37c240307472f97353333038b9623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c804b894530a47eda3978285318ee20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81106053c908430fb5daea01092eadd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/490 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for chunk in df_chunks[len(complete_chunks):len(df_chunks)]:\n",
    "    pruned_chunk = chunk.join(chunk.progress_apply(lambda x: \n",
    "                                                   pd.Series(get_wordcounts(x['book title'], x['book author'], \n",
    "                                                                            books)), axis=1)).dropna(axis=1,thresh=5)\n",
    "    complete_chunks.append(pruned_chunk)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ccea8c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 45023)\n",
      "(1000, 45217)\n",
      "(1000, 42853)\n",
      "(1000, 42734)\n",
      "(1000, 43667)\n",
      "(1000, 44051)\n",
      "(1000, 42776)\n",
      "(1000, 43373)\n",
      "(1000, 44100)\n",
      "(1000, 43991)\n",
      "(1000, 44139)\n",
      "(1000, 44299)\n",
      "(1000, 44267)\n",
      "(1000, 43423)\n",
      "(1000, 43539)\n",
      "(1000, 44323)\n",
      "(1000, 44273)\n",
      "(1000, 44390)\n",
      "(1000, 43432)\n",
      "(1000, 39630)\n",
      "(490, 29334)\n"
     ]
    }
   ],
   "source": [
    "for chunk in complete_chunks:\n",
    "    print(chunk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "150e59df",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 160. KiB for an array with shape (1, 20490) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m complete_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplete_chunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\Genres\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\Genres\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:381\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    159\u001b[0m \"\"\"\n\u001b[0;32m    160\u001b[0m Concatenate pandas objects along a particular axis.\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m 1   3   4\n\u001b[0;32m    367\u001b[0m \"\"\"\n\u001b[0;32m    368\u001b[0m op = _Concatenator(\n\u001b[0;32m    369\u001b[0m     objs,\n\u001b[0;32m    370\u001b[0m     axis=axis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     sort=sort,\n\u001b[0;32m    379\u001b[0m )\n\u001b[1;32m--> 381\u001b[0m return op.get_result()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\Genres\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    612\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    614\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 616\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy:\n\u001b[0;32m    620\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\Genres\\lib\\site-packages\\pandas\\core\\internals\\concat.py:233\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    231\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_join_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\Genres\\lib\\site-packages\\pandas\\core\\internals\\concat.py:577\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m    574\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m ensure_block_shape(concat_values, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_axis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat_values\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\Genres\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py:151\u001b[0m, in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[0;32m    148\u001b[0m             to_concat \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m to_concat]\n\u001b[0;32m    149\u001b[0m             kinds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m--> 151\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kinds \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# GH#39817\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBehavior when concatenating bool-dtype and numeric-dtype arrays is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated; in a future version these will cast to object dtype \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    161\u001b[0m     )\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 160. KiB for an array with shape (1, 20490) and data type float64"
     ]
    }
   ],
   "source": [
    "complete_df = pd.concat(complete_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483ecba",
   "metadata": {},
   "source": [
    "Still too big! Time to switch to Dask! \n",
    "\n",
    "Managed to save them as csvs before the memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6579ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
